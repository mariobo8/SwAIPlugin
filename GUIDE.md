# Training Data Pipeline for SolidWorks AI

## ğŸ¯ Overview

This document describes the automated "Walk & Talk" training data generation system for the SolidWorks AI Assistant. The system opens existing SolidWorks parts, replays their construction history step-by-step, captures visual and mathematical data, and uses GPT-4o to synthesize training pairs.

## ğŸ—ï¸ Architecture

### Component 1: The Batch Walker (C# / SolidWorks API)
- **Location**: `SwAIPlugin/FeatureWalker.cs`
- **Role**: Automates SolidWorks to extract training data
- **Process**:
  1. Opens `.SLDPRT` files from `Training_Input_Parts/`
  2. Rolls back the Feature Tree to the beginning
  3. Steps forward one feature at a time
  4. For each step, captures:
     - `before.png` - Screenshot before the feature
     - `after.png` - Screenshot after the feature
     - `data.txt` - Raw metadata (dimensions, sketch coordinates, types)

### Component 2: The Translator (Python / OpenAI API)
- **Location**: `Backend/process_dataset.py`
- **Role**: Converts raw data into training JSONL
- **Process**:
  1. Reads folders from `Training_Output_Data/`
  2. Sends images + metadata to GPT-4o
  3. GPT-4o generates:
     - Natural language user request
     - Corresponding JSON command
  4. Outputs `train.jsonl` in OpenAI fine-tuning format

---

## ğŸ“ Directory Structure

```
SwAIPlugin/
â”œâ”€â”€ Training_Input_Parts/     # Drop .SLDPRT files here
â”‚   â””â”€â”€ (your parts go here)
â”‚
â”œâ”€â”€ Training_Output_Data/     # Generated by FeatureWalker
â”‚   â””â”€â”€ PartName/
â”‚       â”œâ”€â”€ 000_FeatureName/
â”‚       â”‚   â”œâ”€â”€ before.png
â”‚       â”‚   â”œâ”€â”€ after.png
â”‚       â”‚   â”œâ”€â”€ data.txt
â”‚       â”‚   â””â”€â”€ training_pair.json  # Added by Python script
â”‚       â”œâ”€â”€ 001_FeatureName/
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ walker_log.txt
â”‚
â”œâ”€â”€ train.jsonl               # Final training data file
â”‚
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ process_dataset.py    # The Translator script
â”‚   â”œâ”€â”€ server.py
â”‚   â””â”€â”€ .env                  # API keys (create this!)
â”‚
â””â”€â”€ SwAIPlugin/
    â”œâ”€â”€ FeatureWalker.cs      # The Batch Walker
    â”œâ”€â”€ SwAddin.cs            # Updated with new button
    â””â”€â”€ ...
```

---

## ğŸš€ Quick Start

### Step 1: Setup Environment

1. **Create `.env` file** in `Backend/` folder:
   ```env
   OPENAI_API_KEY=sk-your-openai-api-key-here
   ```

2. **Install Python dependencies**:
   ```bash
   cd Backend
   pip install -r requirements.txt
   ```

3. **Build the C# plugin** in Visual Studio (Debug mode)

4. **Register the plugin** with SolidWorks:
   ```cmd
   regasm /codebase SwAIPlugin\bin\Debug\SwAIPlugin.dll
   ```

### Step 2: Prepare Input Parts

Drop one or more `.SLDPRT` files into the `Training_Input_Parts/` folder.

**Tip**: Start with simple parts that have 3-5 features to verify the pipeline works.

### Step 3: Generate Raw Training Data (C#)

1. Open SolidWorks
2. Go to the **AI Assistant** menu in the toolbar
3. Click **"Generate Training Data"**
4. Confirm when prompted
5. Wait for processing to complete

Check `Training_Output_Data/` for the generated folders.

### Step 4: Process with GPT-4o (Python)

```bash
cd Backend
python process_dataset.py
```

This will:
- Read all step folders from `Training_Output_Data/`
- Send each step to GPT-4o for analysis
- Generate `train.jsonl` in the project root

### Step 5: Verify Output

1. Open `train.jsonl` and inspect the entries
2. Each line should be a valid JSON object with:
   ```json
   {
     "messages": [
       {"role": "system", "content": "..."},
       {"role": "user", "content": "Create a 50mm tall extrusion..."},
       {"role": "assistant", "content": "{\"action\": \"create\", ...}"}
     ]
   }
   ```

---

## ğŸ“Š Data Format

### Raw Metadata (`data.txt`)

```
===== FEATURE METADATA =====
Name: Boss-Extrude1
Type: Boss-Extrude
Suppressed: False

===== FEATURE DEFINITION =====
Direction1:
  EndCondition: Blind
  Depth: 0.025000 m (25.000 mm)
  DraftAngle: 0.00Â°
IsBossFeature: True

===== SKETCH DATA =====
Total Segments: 4

Line 1:
  Start: (0.000, 0.000) mm
  End: (100.000, 0.000) mm
  Length: 100.000 mm
...

===== UNIT CONVERSION NOTES =====
SolidWorks internal units are METERS.
To convert to MILLIMETERS: multiply by 1000
```

### Training Pair (`training_pair.json`)

```json
{
  "user_text": "Create a 100x50mm rectangular extrusion that is 25mm tall",
  "assistant_json": {
    "action": "create",
    "type": "box",
    "parameters": {
      "width": 100,
      "height": 50,
      "depth": 25,
      "units": "mm"
    }
  }
}
```

### Final Training Format (`train.jsonl`)

```jsonl
{"messages":[{"role":"system","content":"You are a SolidWorks AI assistant..."},{"role":"user","content":"Create a 100x50mm rectangular extrusion that is 25mm tall"},{"role":"assistant","content":"{\"action\":\"create\",\"type\":\"box\",\"parameters\":{\"width\":100,\"height\":50,\"depth\":25,\"units\":\"mm\"}}"}]}
```

---

## âš™ï¸ Configuration

### FeatureWalker Settings (C#)

In `FeatureWalker.cs`:
- `SkipFeatureTypes` - Features to ignore (system features)
- `GeometryFeatureTypes` - Features to capture

### Process Dataset Settings (Python)

In `process_dataset.py`:
- `MODEL = "gpt-4o"` - Vision model for analysis
- `MAX_RETRIES = 3` - Retry attempts for API calls
- `SYNTHESIS_SYSTEM_PROMPT` - Instructions for GPT-4o

---

## ğŸ”§ Troubleshooting

### "No .SLDPRT files found"
- Ensure files are in `Training_Input_Parts/` (not a subfolder)
- Check file extension is `.SLDPRT` or `.sldprt`

### "Could not open part"
- The part file may be corrupted or from an incompatible SolidWorks version
- Try opening the part manually in SolidWorks first

### "No geometry features found"
- The part only contains system features (planes, origin, etc.)
- Add at least one extrusion, cut, or other geometry feature

### "OPENAI_API_KEY not found"
- Create a `.env` file in the `Backend/` folder
- Add: `OPENAI_API_KEY=sk-your-key-here`

### Screenshots are black/empty
- Ensure SolidWorks graphics are fully loaded before running
- Try increasing wait time between feature toggles

---

## ğŸ“ˆ Best Practices

### Part Selection
- Use parts with 3-10 features for optimal training data
- Include diverse feature types (extrusions, cuts, fillets, holes)
- Avoid parts with complex assemblies or external references

### Quality Control
- Review generated `training_pair.json` files
- Delete or fix low-quality entries before final processing
- Manually edit `user_text` for more natural phrasing if needed

### Scaling Up
- Process parts in batches of 10-20 at a time
- Monitor OpenAI API usage and costs
- Consider caching results to avoid reprocessing

---

## ğŸ’° Cost Estimation

GPT-4o pricing (as of 2024):
- Input: ~$2.50 per 1M tokens
- Output: ~$10.00 per 1M tokens
- Images: ~$0.003 per image (high detail)

For a typical part with 5 features:
- 10 images Ã— $0.003 = $0.03
- ~2000 tokens input Ã— 5 = ~$0.025
- ~500 tokens output Ã— 5 = ~$0.025

**Approximate cost per part: $0.08 - $0.15**

---

## ğŸ“‹ Next Steps After Generation

1. **Review Quality**: Manually check 10-20 training pairs
2. **Augment Data**: Consider variations (different sizes, positions)
3. **Fine-tune Model**:
   ```bash
   openai api fine_tunes.create -t train.jsonl -m gpt-3.5-turbo
   ```
4. **Integrate**: Update the plugin to use your fine-tuned model

---

## ğŸ“š Technical Notes

### Why GetDefinition() Instead of Macros?
The SolidWorks Macro Recorder generates verbose, specific code that's hard for LLMs to generalize from. Using `GetDefinition()` gives us pure numerical parameters that can be easily formatted into our JSON schema.

### Why Capture Sketch Data?
Feature parameters alone (like "Cut-Extrude") aren't enough. The sketch geometry (circle centers, rectangle corners) provides spatial positioning data that's critical for the AI to learn *where* to place features, not just *what* type they are.

### Unit Handling
SolidWorks API always uses meters internally. The Python script explicitly handles conversion to millimeters in the system prompt to GPT-4o, ensuring generated training data uses user-friendly units.

